{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vzhwD7gOPw0u"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Load and prepare the data\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target.reshape(-1, 1)  # Convert to column vector\n",
        "\n",
        "# One-hot encode the target\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "y_onehot = encoder.fit_transform(y)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_onehot, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalize the data\n",
        "mean = X_train.mean(axis=0)\n",
        "std = X_train.std(axis=0)\n",
        "X_train = (X_train - mean) / std\n",
        "X_test = (X_test - mean) / std"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Neural Network Implementation ##\n",
        "\n",
        "class simpleNeuralNetwork:\n",
        "    def __init__(self,input,Hidden,output):\n",
        "      self.W1= np.random.rand(input,Hidden)*0.01\n",
        "      self.b1=np.zeros((1,Hidden))\n",
        "      self.W2=np.random.rand(Hidden,output)*0.01\n",
        "      self.b2=np.zeros((1,output))\n",
        "\n",
        "    def sigmoid(self,x):\n",
        "      return 1/(1+np.exp(-x))\n",
        "\n",
        "    def sigmoid_derivative(self,x):\n",
        "      return x*(1-x)\n",
        "\n",
        "    def softmax(self,x):\n",
        "      exp_x=np.exp(x-np.max(x,axis=1,keepdims=True))\n",
        "      return exp_x/np.sum(exp_x,axis=1,keepdims=True)\n",
        "\n",
        "    def forward(self,x):\n",
        "      #hidden layer\n",
        "      self.z1=np.dot(x,self.W1)+self.b1\n",
        "      self.a1=self.sigmoid(self.z1)\n",
        "      #output layer\n",
        "      self.z2=np.dot(self.a1,self.W2)+self.b2\n",
        "      self.a2=self.softmax(self.z2)\n",
        "\n",
        "      return self.a2\n",
        "\n",
        "    def CE_loss (self,y_true,y_Pred):\n",
        "      m = y_true.shape[0]\n",
        "      loss=-np.sum(y_true*np.log(y_Pred+1e-15)) / m\n",
        "      return loss\n",
        "    def backward(self, X, y_true, y_pred):\n",
        "        m = y_true.shape[0]\n",
        "\n",
        "        # Output layer gradient\n",
        "        dz2 = y_pred - y_true\n",
        "        dW2 = np.dot(self.a1.T, dz2) / m\n",
        "        db2 = np.sum(dz2, axis=0, keepdims=True) / m\n",
        "\n",
        "        # Hidden layer gradient\n",
        "        dz1 = np.dot(dz2, self.W2.T) * self.sigmoid_derivative(self.a1)\n",
        "        dW1 = np.dot(X.T, dz1) / m\n",
        "        db1 = np.sum(dz1, axis=0, keepdims=True) / m\n",
        "\n",
        "        return dW1, db1, dW2, db2\n",
        "\n",
        "    def update_weights(self, dW1, db1, dW2, db2, learning_rate):\n",
        "        self.W1 -= learning_rate * dW1\n",
        "        self.b1 -= learning_rate * db1\n",
        "        self.W2 -= learning_rate * dW2\n",
        "        self.b2 -= learning_rate * db2\n",
        "\n",
        "    def train(self, X, y, epochs, learning_rate):\n",
        "        losses = []\n",
        "        for epoch in range(epochs):\n",
        "            # Forward pass\n",
        "            y_pred = self.forward(X)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = self.CE_loss(y, y_pred)\n",
        "            losses.append(loss)\n",
        "\n",
        "            # Backward pass\n",
        "            dW1, db1, dW2, db2 = self.backward(X, y, y_pred)\n",
        "\n",
        "            # Update weights\n",
        "            self.update_weights(dW1, db1, dW2, db2, learning_rate)\n",
        "\n",
        "            if epoch % 100 == 0:\n",
        "                print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
        "\n",
        "        return losses\n",
        "\n",
        "    def predict(self, X):\n",
        "        y_pred = self.forward(X)\n",
        "        return np.argmax(y_pred, axis=1)\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "K6dUMPig2GBF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and train the network\n",
        "input_size = X_train.shape[1]\n",
        "Hidden_size = 5\n",
        "output_size = y_train.shape[1]\n",
        "\n",
        "nn = simpleNeuralNetwork(input_size, Hidden_size, output_size)\n",
        "losses = nn.train(X_train, y_train, epochs=1000, learning_rate=0.1)\n",
        "\n",
        "# Evaluate\n",
        "train_preds = nn.predict(X_train)\n",
        "test_preds = nn.predict(X_test)\n",
        "\n",
        "train_accuracy = np.mean(np.argmax(y_train, axis=1) == train_preds)\n",
        "test_accuracy = np.mean(np.argmax(y_test, axis=1) == test_preds)\n",
        "\n",
        "print(f\"rain Accuracy: {train_accuracy:.2f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GhKifw3-Bce",
        "outputId": "7a21e349-4e75-42f0-df60-3e3f10952a88"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 1.0987\n",
            "Epoch 100, Loss: 1.0982\n",
            "Epoch 200, Loss: 1.0800\n",
            "Epoch 300, Loss: 0.8113\n",
            "Epoch 400, Loss: 0.5467\n",
            "Epoch 500, Loss: 0.4434\n",
            "Epoch 600, Loss: 0.3816\n",
            "Epoch 700, Loss: 0.3344\n",
            "Epoch 800, Loss: 0.2953\n",
            "Epoch 900, Loss: 0.2617\n",
            "rain Accuracy: 0.96\n",
            "Test Accuracy: 0.97\n"
          ]
        }
      ]
    }
  ]
}